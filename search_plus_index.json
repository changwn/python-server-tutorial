{"./":{"url":"./","title":"Introduction","keywords":"","body":"python-server-tutorial A book about web server https://sanyuesha.com/python-server-tutorial © 三月沙 all right reserved，powered by GitbookUpdated at 2019-05-23 11:02:59 "},"book/ch00.html":{"url":"book/ch00.html","title":"写在开始之前","keywords":"","body":"写在开始之前 这本小书不是什么: 它不是一本关于 TCP/IP 协议的书，想要对 TCP/IP 协议有全面了解请参考 TCP/IP详解三卷册 它不是一本关于 HTTP 协议的书，想要对 HTTP 协议全面了解请参考 HTTP权威指南 它不是一本全面介绍 socket 编程的书，想要对 socket 编程有全面了解请参考 UNIX网络编程 卷1：套接字联网API 这本小书是什么: 它使用 Python 来介绍 socket 编程，主旨是关于 Web Server 的 为了能最直观地说明 Web Server 中的 socket 编程，一切都按照最简单的模式来处理，比如没有容错、没有上层应用协议处理和解析(HTTP)、没有夸平台支持(只支持 linux)、没有边界检查等 它描述了 web server 中 socket 编程的逐步演化的过程：单进程、多进程、多线程、I/O多路复用以及它们的利弊 读完它，你应该对 web server 中 socket 部分有初步的映像和了解，知道如何探索你使用的 Web Server 的原理，甚至可以根据具体场景改进它 它是活的，我们可能会一直修正它，我们是 wecatch。 © 三月沙 all right reserved，powered by GitbookUpdated at 2019-05-23 11:02:59 "},"book/ch01.html":{"url":"book/ch01.html","title":"基础概念","keywords":"","body":"1. 基础概念 1.1 什么是 socket socket 准确地说是一组应用程序接口，通过这组接口，进程间可以通过类似 unix 标准文件输入(read)输出(write)的方式进行通信。 socket 描述的就是进程间这种通信。 1.2 TCP/IP 协议族 TCP/IP 协议族是一组规定数据如何在网络中进行传输的规约，简单来说它包含传输层、网络层、链路层等不同的通信协议，其中传输层的 TCP/UDP 协议和网络层的 IP 协议是网络编程中最基础、最常见的协议。 一般意义上的网络编程只需要关心 TCP、UDP、IP、HTTP 等协议即可。 1.3 什么是 TCP TCP 全称是 Transmission Control Protocol，是建立在 IP 地址、端口、unix stand file 之上的一种全双工传输层协议，TCP 的主要特点是： 面向连接 提供稳定有序可靠的数据传输（通过诸如超时重发、ack 确认、拥塞控制等策略） TCP 在 socket 编程中是 stream socket，unix socket 中用 SOCK_STREAM 表示。 详见 http://tools.ietf.org/html/rfc793。 1.4 什么是 UDP UDP 全称是 User Datagram Protocol，是一种简单的面向数据报的传输层协议，UDP 不提供类似 TCP 的数据可靠性保证，因而是一种简单的不可靠传输协议，正是由于 UDP 的简单，它的最大特点就是快。 UDP 在 socket 编程中是 Datagram Sockets，unix socket 中用 SOCK_DGRAM 表示。 详见 http://tools.ietf.org/html/rfc768。 1.5 什么是 IP IP 是一种唯一的用来辨识计算机在网络中的位置，同时 IP 也是一种协议，是计算机网络协议中的重要组成部分。 IP 分为 4 字节的 IPV4 和 16 字节的 IPV6，前者能够表达的计算机地址是：2^32=4,294,967,296，后者是前者的 2^96 倍。 IPV4 一般用点分法的 10 进制表示，如 192.168.0.1，IPV6 一般用冒号分割的 16 进制表示 2001:0db8:c9d2:0012:0000:0000:0000:0051。 详见 https://tools.ietf.org/html/rfc791。 1.6 七层网络模型 数据是怎么从 socket 通过协议传送的？答案就是封装 Data Encapsulate。 我们称一个需要传输的数据为一个 packet，一个 packet 需要传递到网络中的另一个进程， 首先这个 packet 需要用应用层协议进行封装，这里是七层模型中的应用层：Application layer (为了叙述方便这里把传输层上面的协议统称为应用层，七层简化为四层)，接着封装之后的 packet 继续被下层协议封装，比如下层是 TCP 协议，七层模型中的传输层：Transport layer，然后再被下层协议封装，比如 IP 协议，七层模型中的：Network layer，最后数据通到达网卡等硬件设备，七层中的物理层：Physical layer。 Application （HTTP） Presentation Session Transport (TCP、UDP) Network （IP） Data Link Physical 当 packet 到达网络的另一端，此时又需要对封装之后的 packet 实行相反的过程进行称之为拆包，从而最终获得原始的 packet。 详见: https://zh.wikipedia.org/wiki/OSI%E6%A8%A1%E5%9E%8B http://www.ha97.com/3215.html 1.7 字节 为了表述方便，计算机通信数据大小统一以字节为单位，1 byte=8 bit。 1.8 字节序 数据在网络中是 big-endian （network byte order）的方式存储，即高位在前的存储方式，机器的内存中是以 little-endian （host byte order）的方式存储，即低位在前的方式存储，为了让网络中的字节序和机器本地的字节序一致，必须进行相互转换，为什么？查看详见。 unix 提供转换的函数是： htons() host to network short htonl() host to network long ntohs() network to host short ntohl() network to host long 详见: http://www.ruanyifeng.com/blog/2016/11/byte-order.html http://blog.erratasec.com/2016/11/how-to-teach-endian.html#.WVtQKRN94S4 1.9 文件描述符 文件描述符是一个简单的非负整数，用来表示一个打开的文件，Unix 中一切都是文件，socket 本质上也可以理解为一个文件。 1.10 计算机是如何交流的 人因为需要交流和沟通，才有了语言，计算机需要交流和沟通，才有了协议。人的交流促成了社交网络，计算机的交流促成了计算机网络，而计算机交流的本质其实是进程的交流，不同计算机上的进程通过 socket 相互通信，socket 提供了进程交流的通道，借助于协议，不同进程说的话才能被另一个进程理解。 计算机就是利用 socket，通过一组组的协议，以太网协议、IP 协议、TCP/UDP 协议进行相互交流的。 © 三月沙 all right reserved，powered by GitbookUpdated at 2019-05-23 11:02:59 "},"book/ch02.html":{"url":"book/ch02.html","title":"最简单的 Web Server 模型","keywords":"","body":"2. 最简单的 Web Server 模型 Client-Server 是互联网上使用最普遍的 Web 模型： 而隐藏在这个模型之后的底层实现就是进程之间通过 socket 进行通信，为了简化我们的描述，这里我们假设只有一个 client 和一个 server 的情形，也就是说 server 只有一个进程存在，client 也只有一个进程存在，那么如何开启两个进程 client 和 server 的通信？ 2.1 server 端的 socket Server 端的 socket 实现过程比 client 端要复杂很多，但是一般的处理过程都可以分为如下几步： 声明 server 的 socket 类型 s = socket.socket(socket.AF_INET, socket.SOCK_STREAM) 首先根据 server 的类型(TCP/UDP)声明对应的 socket，SOCK_STREAM 表示 TCP 协议。 绑定到指定的 ip 和 port server_address = ('localhost', 8000) s.bind(server_address) 把声明的 socket bind 到指定的 ip 和端口，在此读者只需要知道一个 socket 是由其绑定的 ip 和 port 唯一确定的，而一个完整的 socket 通信又是由 client 的 (client_ip, client_port) 以及 server 的 (server_ip, server_port) 唯一确定的。 进行端口侦听 s.listen() 告诉操作系统这个进程开始监听来自绑定 ip 和 port 的请求。 接受 client 的请求进行处理 client, address = s.accept() 完整代码如下： # server.py # create tcp server s = socket.socket(socket.AF_INET, socket.SOCK_STREAM) # enable reuse address port s.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1) # bind socket to the port server_address = ('localhost', 8000) s.bind(server_address) s.listen(backlog) while 1: print('waitting to recevie message from client') connection, address = s.accept() data = connection.recv(1024) print(data) 2.2 Client 端的 socket Client 的 socket 比较简单，首先根据需要 socket 的类型创建 socket，然后 connect 到 server 即可： 完整代码如下： # client.py s = socket.socket(socket.AF_INET, socket.SOCK_STREAM) server_address = ('localhost', 8000) print 'connect to server {0}'.format(server_address) s.connect(server_address) s.send('data') 2.3 Server 是如何处理来自 Client 连接 在 server socket 开始 accept 之后，如果有新的 client 连接进来，accept 会返回一个新的 socket，在这里为了避免和我们之前创建的 socket 对象 s 产生歧义，我们称之为一个 connection。 这个 connection 含义是什么？ 准确的说它是一个 server socket 和 client socket 的连接通道，由 (server_ip, server_port, client_ip, client_port , protocol) 唯一标识的一个特殊的 socket，在这里 protocol 是我们在创建 socket 时使用的 socket 类型，在上文的代码中用 socket.SOCK_STREAM 表示，意思是 TCP 协议类型的 socket。 通过这个 connection，client 和 server 就能相互通信和传输数据，这就是最简单的进程间的通信。 2.4 这个最简单的模型有什么问题 在上文的模型中，server 进程通过监听端口与外界 client 进程交流的过程是阻塞的，而且一次只能处理和一个 client 的对话，处理完之后才能去处理下一个，也就是它的并发处理能力是 1，只要有一个 client 和它建立了 connection，其它的 client 就需要等待。 为什么是这样的？因为只有一个进程在工作，它既需要监听新的 connection ，又需要处理已经建立连接的 connection 。那如何提高它的并发处理能力呢，也就是有没有办法让 server 能够和多个 client 进行交流，当然有，其中一种办法就是多进程 Web Server 模型。 © 三月沙 all right reserved，powered by GitbookUpdated at 2019-05-23 11:02:59 "},"book/ch03.html":{"url":"book/ch03.html","title":"多进程模型","keywords":"","body":"3. 多进程模型 多进程模型是为了解决单个进程既要执行监听工作，又要执行新 socket 的数据传输工作，一人分饰两个角色的低效而诞生的。 3.1 多进程是如何工作的？ 每当有新的 connection 进来，侦听进程在获得 connection 之后 fork 一个新进程来处理这个新的 connection，原有的进程就不必等待处理工作完成之后再去继续监听，而是在 fork 完成之后就可以，这样监听和处理就分开了，各司其职。 在 linux 中，fork 出来的进程和父进程拥有同样的上下文信息、内存数据、与进程关联的文件描述符等，这里的文件描述符中就包含新来的 socket ，不同的是这两个进程完全是在两个不同的虚拟内存段上，相互不干扰。 # create tcp socket s = socket.socket(socket.AF_INET, socket.SOCK_STREAM) # enable reuse address port s.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1) # bind socket to the port server_address = ('localhost', 8000) s.bind(server_address) s.listen(10) while 1: print_green(' waitting to recevie message from client') connection, address = s.accept() if not os.fork(): # enter child process time.sleep(3) # exec task in 3 seconds msg = connection.recv(1024) print_red(\"child process\") connection.send(msg.capitalize()) connection.close() # close client socket s.close() # child does not need this break # break child while loop connection.close() # parent does not need this 在多进程模型中，子进程是拥有与父进程完全一样的上下文信息，由于子进程这个工作进程只用来处理新的 socket 连接 也就是 connection，所以子进程中 copy 过来的监听 socket （负责 accept 的工作）需要释放，而且子进程在处理完毕新的 connection 之后也需要释放自身占用的资源，即退出子进程。 通过简单的多进程模型，处理工作和监听请求就分开了，这样 server 进程可以同时处理多个对话了，也就是我们常说的支持并发，而并发就是同时可以处理多个事情。 多进程模型可以说是最简单的并发模型，在现实的应用当中也有不少开源库采用这种模式，比如 Python 中著名的异步任务队列 RQ ，其工作原理就是对每个需要执行的任务 fork 新的进程来处理。 3.2 多进程的问题 多进程能够解决单进程模型中不能并发的问题，而且实现简单直观，同样多进程也有其自身的缺点。 3.2.1 数据共享复杂 在真实的 Web Server 中，经常需要工作单元和监听单元进行数据共享，比如 Web Server 的某些全局状态(访问量统计，session 的修改，白名单黑名单的设置等)可能会因为部分请求而改变，如果 Web Server 是多进程模型，此时这个共享的全局状态就应该及时周知 Web Server 中正在处理新 connection 的所有子进程，在多进程模型中处理类似的问题就需要进程间的通信。 进程间通信有多种方式，常见的有：信号、共享内存，还有我们现在正在讨论的 socket。 信号 信号是 linux 中两个进程进行通信的一种重要手段，但是相比在 Web Server 中需要交流沟通的两个进程来说，由于 linux 中的信号是由一组有限的信号函数组成，信号能够表达和发送的数据信息都非常简单，无法满足 Web Server 中复杂的数据共享需求。 共享内存 共享内存，是指两个进程共享一段物理内存，相比信号，共享内存能够提供更复杂的数据交换方式；但是共享内存却有一个无法忽视的缺点：共享数据的访问的安全性必须由开发者来保证，因为它没有一个很好的同步机制来保证同一段内存信息被多个进程访问时的读写安全。 socket 现在假设，工作进程和监听进程是通过 socket 来共享上数据的，也就是说监听进程获取的新的 socket 可能是来自一个自身 fork 出来的子进程，目的是为了告诉它一些共享数据的变化。为了能够区分新到的 socket 是来自子进程还是其他的正常网络请求，监听进程必须要做一些额外的工作，比如正确解析来自子进程的协议，为了能够及时通知其他正在工作的子进程，监听进程又需要保留所有工作子进程的相关信息在以便数据发生改变时通知它们。虽然到这步我们需要处理的工作还远未结束，但是已经到可以感受到这种方式的复杂性了，比如子进程还需要和父进程保持连接并且保证能随时收到来自父进程的通知等等。 socket 的模式真的很复杂，使用 socket 模式就像 Web Server 中本身又需要一个 Web Server 来接管和子进程的通信，现实中这部分需求往往我们会借助第三方工具来实现，比如 MQ。 3.2.2 能够支持的并发非常有限 多进程模型除了带来进程间通信交流的复杂问题，并发量大时还会占用大量操作系统资源。在 linux 中每多一个进程处理，计算机就要分配相应的资源对进程进行管理，比如进程的上下文切换，进程的唤醒和中断等，僵死进程的处理等诸如此类的问题。 试着把你的 chrome 浏览器标签页开到几十个上百个感受一下多进程带来的直接影响。 图中是进程模式 Web Server 下 4 个 client，每个请求间隔 3s 发起，持续约 1 分钟左右的请求导致的 server 端 CPU 利用率曲线。 多进程模型已经能够简单的支持并发了，却又引入其他更复杂的问题，那还有更好的机制么？当然有，那就是我们接下来要说的多线程模型 Web Server。 © 三月沙 all right reserved，powered by GitbookUpdated at 2019-05-23 11:02:59 "},"book/ch04.html":{"url":"book/ch04.html","title":"多线程模型","keywords":"","body":"4 多线程模型 多进程模型 Web Server 带来一个难题：进程间的数据共享，而多线程 web server 处理这个问题显然更得心应手，因为同一个进程的多个线程是共享上下文的。 要想把这个问题说清楚，我们首先说说线程是什么以及它的特点。 4.1 线程 线程是操作系统的一个调度单元，线程包含在进程之中。一个进程最少有一个线程，叫主线程，也就是运行中的进程。一个进程中可以有多个线程，线程之间共享进程的全部系统资源，比如程序数据、文件描述符等等。线程可以挂起、中断、结束，也就是操作系统可以通过这些状态来对进程中的多个线程进行调度，比如某个线程有 IO 等待时就可以暂时被挂起，让出 CPU 让其他线程执行。 正式基于线程有这样的特点，多线程的 Web Server 共享数据比多进程的更容易实现，而且效率更高。 def process_client_request(client): time.sleep(3) # exec task in 3 seconds msg = client.recv(1024) print_red(\"thread %s\" % threading.current_thread().name) client.send(msg.capitalize()) client.close() # close client socket def main(): # create tcp socket s = socket.socket(socket.AF_INET, socket.SOCK_STREAM) # enable reuse address port s.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1) # bind socket to the port server_address = ('localhost', 8000) s.bind(server_address) s.listen(10) while 1: print_green( str(datetime.now()) + ' waitting to recevie message from client') client, address = s.accept() t = threading.Thread(target=process_client_request, args=(client,)) t.daemon = True t.start() 可以看出在线程中很容易获得进程的所有状态信息以及数据，但是既然是每个线程都可以很方便的获取进程的数据，势必就存在多线程同时读写同一数据的情况，这就又引入一个问题：共享资源的读写安全，也就是我们经常说的线程安全 thread-safe。 4.2 什么是线程安全？ 如果一段代码或共享数据在多线程之下运行，并且总是能保证运行结果是正确的，它就是线程安全的，换句话说就是必须保证在任意时刻只有一个线程对内存的同一快区域进行操作。 如何保证线程安全？方法当然是有的，是不是每一种都适合用于多线程模型的 web server 中呢，我们一一来讲。 第一种，不对全局共享数据做修改操作 如果你不做任何保护措施，不要在多线程场景中对全局共享数据做修改操作。这样当然能保证是线程安全的，但这显然不是我们的 web server 中需要的，因为我们的目的就是要做数据共享并且就是要修改全局数据。 第二种，线程本地数据 thread local storage 顾名思义，thread local storage 是指把数据当成当前线程自身数据的一部分，对于其它线程来说是一个私有变量，只有创建数据的线程能够进行读写。这种方式之下，多个线程之间无法共享彼此的 local storage，这不是我们希望的 Web Server 所看到的。 第三种，互斥锁 mutual exclusion lock 互斥锁就是使用某种机制来保证在任意时刻证共享资源被访问时只能有一个线程，互斥锁的使用必须要非常小心，因为使用不当可能会导致死锁、线程饥饿等现象。可见互斥锁是提供了一种保护机制，但是各个线程之间还是可以共享数据的，只是需要对这部分数据做好保护即可，好像可以用解决我们遇到的问题哦。 第四种，原子操作 原子操作是指正在执行的线程的操作不会被其它线程中断，之所以是原子就是因为这样的操作是最小的执行单元了，已经无法在分割。原子操作必须在系统层面做支持，而我们 web server 中对数据的共享操作几乎都是发生在用户进程中，而且根据场景的不同操作方式也是各一，能够使用原子操作的地方还要视具体场景而定，限制较多。 综上，使用互斥锁是对共享数据进行加锁保护是一种可行的机制，因而可以再多线程 web server 中采用。 修改上面的例子: lock = threading.Lock() GLOBAL_REQUEST_STATE = { 'count': 0 } def process_client_request(client): time.sleep(3) # exec task in 3 seconds msg = client.recv(1024) print(\"thread %s\" % threading.current_thread().name) client.send(msg.capitalize()) client.close() # close client socket with lock: GLOBAL_REQUEST_STATE['count'] += 1 def main(): # create tcp socket s = socket.socket(socket.AF_INET, socket.SOCK_STREAM) # enable reuse address port s.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1) # bind socket to the port server_address = ('localhost', 8000) s.bind(server_address) s.listen(10) while 1: print( str(datetime.now()) + ' waitting to recevie message from client') client, address = s.accept() t = threading.Thread(target=process_client_request, args=(client,)) t.daemon = True t.start() global_request_state 全局变量，每一个 request thread 都会修改，因而用锁进行保护。 加锁保护是多线程模型中必要的数据保护机制，不仅仅用在多线程的 web server 中，只要在多线程模型中存在多个线程对同一个内存区域进行读写就需要锁的保护。 4.3 多线程的问题 多线程的为 Web Server 提供了请求线程资源共享的能力，那我们的 Web Server 模型是不是就没有缺点了？并不是。 到目前为止，我们所用的 socket 都是阻塞式 blocking 的，也就是无论是 accept 还是 send、recv 都会导致当前工作进程或线程由于消息没有及时发出或到达而处于等待状态，这样的场景在多并发之下更是非常普遍。 我们一起来设想一下。 在多线程 Web Server 中，某个工作线程处理的 socket 由于某种原因消息没有送达，暂且叫它 B 线程，调用 recv 时发生阻塞，由于线程的调度是操作系统控制的，此时 B 线程由于不再占用 CPU 而被操作系统挂起，让出 CPU 的控制权给其它工作线程使用，某一刻 B 线程等待的 socket 数据到达了可以读取了，但是 B 线程何时才能获得 CPU 的使用权从而读取数据？答案是不知道。因为线程的调度完全是随机的，任由操作系统根据每个线程占用时间片和线程当前的状态进行调度，也就是 B 线程完成数据的读取的时机是不确定的。在真实的 Web Server 工作环境中，类似 B 线程的这种线程是大量存在的。那么我们该如何准确的准确无误的唤醒等待中的线程告诉它等待的 socket 可写可读了，让它们赶紧干活呢？ 答案就是 IO 多路复用技术 -- IO multiplex。 © 三月沙 all right reserved，powered by GitbookUpdated at 2019-05-23 11:02:59 "},"book/ch05.html":{"url":"book/ch05.html","title":"IO 多路复用技术","keywords":"","body":"5. 多路复用技术 5.1 为什么需要 IO 多路复用技术？ 在多线程 web server 模型下由于线程的调度切换完全是由操作系统控制的，如果 socket 在某一时刻不可读或不可写导致调用阻塞引起线程被挂起，而 socket 何时可读何时可写都只能随机等待线程重新获取被调用的机会只会才能知道，还有一种做法就是设手动不断的检查来查看 socket 是否可读可写，然后再调用相应的工作线程进行处理，无论是哪种方式都将引入额外的开销，而且代码将变得非常复杂而且难以维护。 5.2 什么是 IO 多路复用技术？ 在非 IO 多路复用技术的模型中，我们怎么得知有新的 socket 连接或者 socket 已经准备好读或写，只能是一直苦苦等待或者不断做轮询检查，而在 IO 多路复用技术可以做到主动通知。 在 IO 多路复用技术中，通过告知内核需要监听的 socket 以及对应的事件，由内核通知我们 socket 是否已经准备就绪，而不是不断的检查或做无用功。也就是说 IO 多路复用技术是内核提供的一种功能，由于是被动通知，IO 多路复用技术又称事件驱动。 IO multiplex 多用在是网络编程中，常常用来解决一下问题： A Client 同时处理多个文件描述符 A Client 同时处理多个 sockets A TCP Server 同时处理监听 socket 以及已经连接的 socket A Server 同时处理 UDP 和 TCP A Server 同时处理多种服务和多种协议 5.3 IO 多路复用技术用到的基础概念 阻塞是指调用需要等待结果的完成，调用会影响后续指令的进行，可以认为计算机的任何调用都是阻塞的，因为不管什么调用都有 CPU 的执行等待，所以阻塞是一个相对概念，只有在限定场景之下讨论阻塞才更有意义，比如一个请求涉及 5 个函数调用，请求的平均响应时间是 20 ms，分布到每个函数是 4 ms，如果某个函数调用时需要等待的时间导致请求的平均响应时间变长了，我们说这个函数是一个阻塞操作。 非阻塞是指调用方式不影响后续的指令执行，即函数执行之后立即返回，不等待结果的响应。 同步是指调用方需要等待结果的返回，然后才执行后续指令，不管等待时间的长短，同步必然阻塞。 异步是指调用方不需要等待结果的返回，结果以通知的形式获得，真正的异步是需要在程序执行的每一步执行都需异步操作，比如 nodejs 中的 callback 通知。 5.4 IO 模型 IO 多路复用技术会涉及到以下几种 IO 模型的讨论，他们分别是： blocking I/O nonblocking I/O I/O multiplexing (select and poll) signal driven I/O (SIGIO) asynchronous I/O (the POSIX aIO_ functIOns) 所有的 IO 模型都会涉及以下两个基本的执行流程： 等待数据可用，这通常是指等待网络中的数据包到达，到达之后内核把会把数据包 copy 到内核 buffer。 从内核 copy 数据到处理进程，这是指把数据从内核拷贝到用户进程。 5.4.1 阻塞 IO 指程序在遇到 IO 调用时一直等待 IO 就绪才会执行相应的指令，我们在前几章讨论的单进程模型，多线程模型都属于 recv 数据都属于阻塞 IO，同步阻塞是最最简单的模型，socket 默认是阻塞的。 5.4.2 非阻塞 IO 遇到 IO 调用时不管 IO 有没有准备就绪（内核把数据拷贝到进程）都直接返回。 5.4.3 IO multiplex 借助 select 和 poll 等内核提供的机制只在 IO 准备就绪内核通知我们以后才进行 IO 操作。 5.4.4 信号驱动 IO 同 IO 多路复用技术，内核通过信号通知我们的 IO 何时就绪。 5.4.5 Asynchronous IO 同信号驱动 IO，内核通过信号通知我们的 IO 何时就绪，区别在于信号驱动 IO 通知发生在数据开始拷贝时，而异步 IO 通知在数据已经完成从内核到用户进程的拷贝。 在接下来的几个小节会对 linux 提供的几种 IO 多路复用技术一一进行介绍，他们分别是 select、poll、epoll。 © 三月沙 all right reserved，powered by GitbookUpdated at 2019-05-23 11:02:59 "},"book/ch06.html":{"url":"book/ch06.html","title":"IO 多路复用之 poll","keywords":"","body":"6. IO 多路复用之 select 这一节我们说说 O 多路复用的技术 select。 6.1 select 是什么 select 是 linux 内核提供的函数，它允许同时监听多个文件描述符的读写事件，在这些事件中有一个或多个变得可用之后以及经历一段指定的时间之后通知用户进程。 6.2 使用 select Python 中提供了 select 模块来调用系统的 select 函数： select.select(rlist, wlist, xlist[, timeout] select 有 4 个参数，分别是读事件监听对象，写事件监听对象，异常事件对象，超时设置。 在这个例子中，我们要创建一个回显服务，首先我们创建一个 socket 来处理连接监听: server = socket.socket(socket.AF_INET, socket.SOCK_STREAM) server.setblocking(0) server_address = ('localhost', 8000) server.bind(server_address) server.listen(10) 注意在上面的代码中，我们设置了当前的 socket 为非阻塞的 server.setblocking(0)，在 Web Server 中我们始终有一个 socket 是来监听的新的 socket 连接，因而这个监听 socket 要加入读事件队列： inputs = [server] 除此之外我们还需要处理写事件，因而我们需要一个写列表用来处理需要写入数据的 socket： outputs = [] message_queues = {} 我们开始我们的主循环： while inputs: readable, writable, exceptional = select.select(inputs, outputs, inputs) select 函数返回三个文件描述符列表，分别代表有可读事件列表，有可写事件列表，有异常事件列表，返回的这些事件列表中包括了我们作为 select 参数的那些文件描述符。 6.3 读事件列表 读事件列表根据不同的情形有如下三种处理方式。 情形一 如果一个文件描述符可读，而且这个文件描述符恰巧就是我们刚才创建的 server，说明我们侦听的 socket 有新的连接请求： for s in readable: if s is server: connection, client_address = s.accept() connection.setblocking(0) inputs.append(connection) message_queues[connection] = Queue.Queue() 在上面的代码中，我们通过 accept 函数获得新的连接，并且把该连接加入 inputs 列表，等待连接到该 connection 的 client 发送数据，在此我们也给这个新的 connection 创建一个写消息的队列。 情形二 如果一个描述符不是侦听 socket 说明这个描述符有数据可读，此时我们可以使用 recv 来读取数据： else: data = s.recv(1024) if data: if s not in outputs: outputs.append(s) message_queues[s].put(data.capitalize()) 情形三 如果读取的数据是空的，说明 client 关闭了这个连接，我们需要把这个连接从我们的监控列表中移除，并且释放对应的资源： else: if s in outputs: outputs.remove(s) inputs.remove(s) s.close() 6.4 写事件列表 对于写事件列表来说，在这个例子中需要处理的事情比较有限，如果这个描述符有数据要发送，则通过 send 函数发送数据： for s in writable: try: next_msg = message_queues[s].get_nowait() except Queue.Empty: if s in outputs: outputs.remove(s) else: s.send(next_msg) 6.5 异常列表 最后是异常列表，说明描述符有错误发生，关闭它们并且释放资源： for s in exceptional: inputs.remove(s) if s in outputs: outputs.remove(s) s.close() del message_queues[s] 6.6 select 的超时设置 select 的超时设置有以下三种情形： 情形一 一直等待下去，仅在有一个描述符准备好 IO 时返回，此时参数为 null 情形二 等待一段固定的时间，在有一个描述符准备好 IO 时返回，此时参数是大于 0 的 情形三 不等待，检查完描述符之后立即返回，此时参数是 0 6.7 select 的问题 select 是早期 linux 内核提供的多路复用技术，并且伴随着 linux 设计的限制，自身也有些不足： select 有最大监听描述符限制 select 是通过逐个扫描的方式获取有事件的描述符 在多线程环境下，select 的表现不尽如人意 正因为 select 有不足才有了下一节我们要讲的另一个 IO 多路复用技术：poll © 三月沙 all right reserved，powered by GitbookUpdated at 2019-05-23 11:02:59 "},"book/ch07.html":{"url":"book/ch07.html","title":"IO 多路复用之 select","keywords":"","body":"7. IO 多路复用之 poll 这小节我们介绍 IO 多路复用技术的 poll。 7.1 poll 是什么 poll 是类似于 select 的又一个 linux 提供的内核函数，但是 poll 的底层实现和 select 相比更高效，更详细的差异后面的章节会讲到。 7.2 使用 poll Python 中 poll 的实现同样也在 select module 中，但是不同于 select，poll 是通过主动注册文件描述符和事件实现对文件描述符的监听，而且 poll 提供了一些列的事件 flag 来进行对应的事件注册： 事件 含义 POLLIN There is data to read POLLPRI There is urgent data to read POLLOUT Ready for output: writing will not block POLLERR Error condition of some sort POLLHUP Hung up POLLNVAL Invalid request: descriptor not open 同上文中 select 的例子一样，我们使用 poll 创建一个回显服务，区别是 poll 需要主动注册文件描述符和需要监听的事件： READ_ONLY = select.POLLIN | select.POLLPRI | select.POLLHUP | select.POLLERR READ_WRITE = READ_ONLY | select.POLLOUT # Set up the poller poller = select.poll() poller.register(server, READ_ONLY) 由于 poll 返回的是一个文件描述符号和对应事件的列表，所以我们需要提前记住所有文件描述符的 fileno： fd_to_socket = { server.fileno(): server} poll 也有超时设置，不过 poll 的 timeout 单位是 ms，为了让 poll 不会一直阻塞，我们指定 timeout 为 1s： TIMEOUT = 1000 开启主循环，在 timeout 之后检测返回的事件列表和对应的文件描述符： while True: events = poller.poll(TIMEOUT) for fd, flag in events: pass 7.3 读事件 POLLIN 和 POLLPRI poll 的读事件对应的是 POLLIN 和 POLLPRI ，同 select 读事件一样也有以下的几种情形。 情形一 如果文件描述符是 server，说明有新的连接到来，在获取这个连接之后需要把新的连接通过 poll 来进行注册，这样我们就可以监听新连接的事件了： if s is server: connection, client_address = s.accept() connection.setblocking(0) fd_to_socket[ connection.fileno() ] = connection poller.register(connection, READ_ONLY) message_queues[connection] = Queue.Queue() 情形二 如果文件描述符不是 server，说明这个文件描述符有数据可读，此时我们可以使用 recv 来读取数据： else: data = s.recv(1024) if data: message_queues[s].put(data.capitalize()) poller.modify(s, READ_WRITE) 与此同时，我们也需要修改这个文件描述符需要被监听的事件类型，在读完数据之后我们需要进行数据回显，因此修改事件类型为 READ_WRITE，这点与 select 不同。 情形三 如果读取的数据是空的，说明 client 关闭了这个连接，我们需要把这个连接从 poll 的注册列表中移除，并且释放对应的资源： poller.unregister(s) s.close() # Remove message queue del message_queues[s] 7.4 写事件 POLLOUT 对有写事件的文件描述符的处理和 select 类似，区别在于如果没有消息可写，我们修改描述符的监听事件为 READ_ONLY： elif flag & select.POLLOUT: try: next_msg = message_queues[s].get_nowait() except Queue.Empty: poller.modify(s, READ_ONLY) else: s.send(next_msg) 7.5 异常处理 POLLHUP 和 POLLERR POLLHUP 说明 client 没有正确关闭连接，POLLERR 说明有错误发生，这两种情形我们选择取消对文件描述符的监听，并且释放资源： poller.unregister(s) s.close() # Remove message queue del message_queues[s] 7.6 poll 和 select 的对比 select 和 poll 都支持同时监听多个文件描述符，但是他们有着显著的区别。 select select 有最大文件描述符限制 为了获取具体哪个文件描述符有事件可用，select 需要对所有监听的文件描述符进行线性扫描 每次调用 select ，操作系统都需把文件描述符集合在内核和用户空间中相互拷贝 poll poll 没有最大文件描述符限制 为了获取哪个文件描述符，poll 也需要对监听的文件描述符进行线性扫描 © 三月沙 all right reserved，powered by GitbookUpdated at 2019-05-23 11:02:59 "},"book/ch08.html":{"url":"book/ch08.html","title":"IO 多路复用之 epoll","keywords":"","body":"8. IO 多路复用之 epoll 前面介绍了 select 和 poll，这一节我们说说另一种 IO 多路复用技术 epoll。 8.1 epoll 是什么 epoll 是类似于 poll 的又一个 linux 提供的内核函数，正如 poll 的诞生是为了弥补 poll 的缺点，epoll 也是为了 poll 的缺点而实现的。 8.2 使用 epoll 同之前一样创建一个回显服务，在使用 epoll 之前首先创建 socket server 以及开始主循环的一些准备工作： server = socket.socket(socket.AF_INET, socket.SOCK_STREAM) server.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1) server.setblocking(0) server_address = ('localhost', 8000) server.bind(server_address) server.listen(10) message_queues = {} TIMEOUT = 1 进行事件注册和开启主循环： READ_ONLY = select.POLLIN | select.POLLPRI | select.POLLHUP | select.POLLERR READ_WRITE = READ_ONLY | select.POLLOUT poller = select.epoll() poller.register(server.fileno(), READ_ONLY) fd_to_socket = {server.fileno(): server} while True: events = poller.poll(TIMEOUT) for fd, flag in events: pass 可以看到 epoll 几乎有着和 poll 一样的 API，有一点不同的是 epoll 支持的事件更多，而且 epoll 的 timeout 是以秒为单位的，poll 是以毫秒为单位的。 8.2 读事件 POLLIN 和 POLLPRI epoll 的读事件对应的是 POLLIN 和 POLLPRI ，同 poll 读事件几乎一样，同样处理 epoll 读事件主要有以下几种情形。 情形一 如果文件描述符是 server，说明有新的连接到来，在获取这个连接之后需要把新的连接通过 epoll 来进行注册，这样我们就可以监听新连接的事件了： if s is server: connection, client_address = s.accept() connection.setblocking(0) fd_to_socket[ connection.fileno() ] = connection poller.register(connection, READ_ONLY) message_queues[connection] = Queue.Queue() 情形二 如果文件描述符不是 server，说明这个文件描述符有数据可读，此时我们可以使用 recv 来读取数据： else: data = s.recv(1024) if data: message_queues[s].put(data.capitalize()) poller.modify(s, READ_WRITE) 与此同时，我们也需要修改这个文件描述符需要被监听的事件类型，在读完数据之后我们需要进行数据回显，因此修改事件类型为 READ_WRITE。 情形三 如果读取的数据是空的，说明 client 关闭了这个连接，我们需要把这个连接从 poll 的注册列表中移除，并且释放对应的资源： poller.unregister(s) s.close() # Remove message queue del message_queues[s] 8.3 写事件 POLLOUT 对有写事件文件描述符的处理和 poll 一样： elif flag & select.POLLOUT: try: next_msg = message_queues[s].get_nowait() except Queue.Empty: poller.modify(s, READ_ONLY) else: s.send(next_msg) 8.4 异常处理 POLLHUP 和 POLLERR POLLHUP 说明 client 没有正确关闭连接，POLLERR 说明有错误发生，这两种情形我们选择取消对文件描述符的监听，并且释放资源： poller.unregister(s) s.close() # Remove message queue del message_queues[s] 在介绍完 epoll 的基本使用，我们再说说 epoll 的两种执行模式。 8.5 epoll 的水平触发和边缘触发 epoll 有两种执行模式，一种是水平触发（level-triggered），一种是边缘触发（edge-triggered），默认是水平触发。 8.5.1 水平触发 level-triggered 意思是如果一个文件描述符就绪了，也就是有事件准备读或准备写，调用 epoll 之后内核会把这个文件描述符从内核拷贝到用户空间，并通知用户对应的事件，如果用户此时选择不处理该描述符的事件，即用户不进行数据读或不进行数据写，下次调用 epoll 时，内核依然会返回这个事件。 如图所示不论 epoll 调用多少次，只要事件 event2 就绪，内核总是会把事件描述符返回给用户。 8.5.2 边缘触发 edge-triggered 意思是如何一个文件描述符就绪了，也就是有事件准备读或准备写，调用 epoll 之后内核会把这个文件描述符从内核拷贝到用户空间，并通知用户对应的事件，如果用户选择不处理该描述符的事件，即用户不进行数据读或不进行数据写，下次调用 epoll 时，内核不会再返回这个事件，除非把对应的事件重新激活，内核才会在下次调用中返回该事件。 第二次调用 epoll 不会返回已经返回过的事件 event2。 8.6 两种触发模式的差异 level-triggered 描述的是一种状态，一种存在 present，即事件发生了这一种状态，只要这个状态一直存在，内核就会一直不断的处理，也就是同一个事件可能会被内核反复拷贝。 edge-triggered 描述的是一个事件，一个发生的事件 event，即事件从无到有这一状态的改变，也就是发生了这件事，此时内核才会拷贝这个事件，如果用户没有处理，接下来内核不会再次进行拷贝，即使这个事件在内核中存在，但是这个事件已经发生过了。 level-triggered 模式是尽可能的让事件被用户应用程序感知到，而 edge-triggered 可能的问题是也许会会让用户应用程序错过某些事件，因为它只在事件发生的那一刻才会通知用户。 edge-triggered 更高效，但是 level-triggered 更可靠。 8.7 边缘触发的使用 epoll 默认是 level-triggered，如果需要使用 edge-triggered 需要更改注册事件类型以及处理事件的方式。 首先需求修改注册事件类型： poller.register(server.fileno(), READ_ONLY | select.EPOLLET) 其次在需要处理事件时我们必须一次性全部处理完成，accept 处理： try: while True: connection, client_address = s.accept() connection.setblocking(0) fd_to_socket[connection.fileno()] = connection poller.register(connection.fileno(), READ_ONLY) # Give the connection a queue for data we want to send message_queues[connection] = Queue.Queue() print_red(' connection {0} is comming ==> '.format( connection.getpeername())) pass except socket.error: pass 读取数据： data = '' try: while True: _data = s.recv(1024) if not _data: break data += _data except socket.error: pass 发送数据： try: while next_msg: bytelength = s.send(next_msg) next_msg = next_msg[bytelength:] except socket.error: pass © 三月沙 all right reserved，powered by GitbookUpdated at 2019-05-23 11:02:59 "},"book/ch09.html":{"url":"book/ch09.html","title":"select、poll、epoll 如何选择","keywords":"","body":"9. select、poll、epoll 如何选择 linux 提供了 select、poll、epoll 等不同 IO 多路服务用技术来支持高性能的 Web Server 场景，但是它们之间的本质区别在哪里，我们又该如何选择？ 9.1 select select 是 linux 中早期的 IO 多路复用技术，受限于当时的技术场景和要求，select 的在设计之初它的一些特性就无法满足现代互联网高性能 Web Server 的要求： select 是通过 fd_set 来表示需要监听的文件描述符的，即一个整数数组，数组的每一个数字的一位表示一个我们感兴趣的文件描述符，调用 select 需要传递我们感兴趣的文件描述符，它们分别组成了 readset、writeset 和 exceptset ，由于 select 的系统调用会修改这些 fd_set，因为它们既是参数，又是结果集，这意味着即使只有一个文件描述符有事件要处理，整个 fd_set 都需要在内核和应用程序之间进行拷贝 如果想要知道哪一个文件描述符有事件需要处理，需要对整个 fd_set 进行迭代依次做检查，即使仅仅只有一个文件描述符有事件要处理，随着文件描述符数量的增大，其复制的开销也线性增长。同时，由于网络响应时间的延迟使得大量TCP连接处于非活跃状态，但调用 select 却会对所有 socket 进行一次线性扫描 select 支持的最大文件描述符是通过常量 FD_SETSIZE 控制的，而这个常量在 linux 编译时才能修改和确定，意味着 select 有最大文件描述符限制，通常是 1024，如果想要修改这个值，需要重新对 linux 内核进行编译 select 对多线程不友好 select 有这么多缺点，是不是应该将之送进计算机历史博物馆了，有什么理由需要使用 select 么？ 理由一 select 有很好的跨平台特性，由于 select 诞生非常早，select 在大部分平台都得到了支持。 理由二 select 处理 timeout 时的精度要比 poll 和 epoll 要高，如果在某些场景需要更高精度的 timeout 控制，select 是个更好的选择，比如实时性要求很高的场景。 9.2 poll poll 的出现弥补了 select 自身的一些缺陷，它支持并包括： 没有 select 监听文件描述符大小的限制 poll 不会修改传给它的 pollfd ，意味着可以重用每次文件描述符集 poll 对各种 event 的支持更广泛更精细 但是 poll 同样也有自身的缺点： 类似 select，为了找出哪个文件描述符有事件需要处理，poll 需要遍历所有文件描述符 类似 select，poll 不支持动态修改监听中的文件描述符事件 poll 用在什么场景更合适？ 场景一 poll 的跨平台特性比较好，如果跨平台是必须要有的，可以考虑 poll。 场景二 同时需要监听的 socket 少于 1000 ，可以考虑使用 poll。 场景三 同时需要监听的 socket 少于 1000 ，而且这些连接大部分都是短连接。 场景四 没有动态修改事件的需要。 9.3 epoll epoll 是现代高性能网络应用程序的代表，它支持并包括： epoll 只会返回已经触发事件的文件描述符，不需要迭代所有的文件描述符 epoll 可以动态的修改事件，支持在添加和移除监听文件描述符 epoll 在多线程环境下支持良好 epoll 的高效同样也意味着 epoll 是要比 poll 和 select 都更复杂的 IO 多路复用技术，而且更多的特性也就是意味着更多的系统调用开销，比如每次修改事件都需要一次系统调用，由于 epoll 在获取一个新的 connection 时需要两次系统调用，所以在需要处理大量短连接的场景下 epoll 可能会比 poll 花费更多的时间，epoll 仅仅在 linux 平台可用，即便如此，epoll 也是编写高性能网络应用程序的首选，尤其是应用程序是以下情形： 需要处理大量连接 连接中包括大量的长连接 9.4 最简单的选择 当你下次编写网络应用程序面对 select、poll、epoll 几种 IO 多路复用技术犹豫不决时，除非你真的很确定需要的是 select 或 poll，否则总是选 epoll 就是对的。 © 三月沙 all right reserved，powered by GitbookUpdated at 2019-05-23 11:02:59 "},"book/ch10.html":{"url":"book/ch10.html","title":"C10k 问题","keywords":"","body":"10. C10k 问题 C10k (http://www.kegel.com/c10k.html) 问题是一台主机同时支持 10k 并发连接，此文虽然成文于十多年前，但是对理解 linux 中 Web Server 的发展和设计有非常大的帮助，而且作者（Dan Kegel）提供了一系列的文章和资料可供研读，借助本文我们一起看看作者分析 Web Server 问题的思路。 提出问题 作者从现代计算机硬件所能承受的带宽和计算速度来(注:十多年前的计算机)计算支持单个 client 所需要的内存和带宽以及计算周期，从而得出当时的计算机承载 10k 个连接并不是多大的问题，于是作者一步步提出了解决方案。 基础知识准备 在理解作者所要讲解的问题之前，作者提供了一本书供读者参考--Unix Network Programming : Networking Apis: Sockets and Xti (Volume 1)，只有对 socket、IO 模型、线程等操作系统技术有所理解才能更好的理解作者提出的问题。 借力开源 IO framework 作者列举和介绍了一些著名的 IO framework，借助这些 IO framework 开发者可以比较容易实现各种夸平台的高性能的 Web Server，作者的意图也很明显：不要重复发明轮子，程序员要善于借力。 各种 IO 编程范式在各个平台的实现 作者分析了各种 IO 策略在各个平台的实现以及它们的优缺点，其中包括： 使用非阻塞 IO 单线程 level-trigger 模型 使用非阻塞 IO 单线程 edge-trigger 模型 单线程异步 IO 模型 多线程模型 有了这些分析，读者几乎可以说对这些模型的优缺点有了大体的掌握和理解。 特定问题特定分析 总是有些问题需要额外关注，作者列举了 linux 的打开文件的问题，Java 的 IO 模型实现问题，这些问题在设计和实现 Web Server 时都需要特别关注。 技巧和限制 作者列举在实现高性能的的同时有些技巧需要我们去留意，比如 zero-copy、内存映射技术、无锁编程等等，除此之外一些系统或技术的限制我们也应该去了解，也就是说要充分发挥技术的优势，尽量避免技术的缺陷。 如何衡量性能 说了那么多高性能的实用技术，又如何评估呢？作者提供了两个指标： 每秒的连接数 尽可能多的传输大文件的 client 存在时 client 的传输速率 开源实现 最后是各种模型范式的开源实现，这些都是非常好的学习资料，几乎囊括了几乎现在所有的高性能服务的实现模型。 © 三月沙 all right reserved，powered by GitbookUpdated at 2019-05-23 11:02:59 "},"book/ch11.html":{"url":"book/ch11.html","title":"高性能 web server","keywords":"","body":"11. 高性能 web server 在前面的文章中，我们先后介绍了： 单进程模型 多进程模型 多线程模型 IO 多路复用技术 每种模型都有自身的优缺点，都有自己的适用场景，比如单进程模型一般仅仅在开发测试中使用，多进程和多线程模型会用在并发请求不是很多的场景下，比如内部系统，数据库系统等，IO 多路复用技术则在很多高性能 Web Server 中发挥着重要作用，比如我们常见的 nginx。 那如何衡量一个 Web Server 是高性能的，又如何设计出高性能的 Web Server？ 11.1 高性能 Web Server 的衡量指标 可量化是衡量性能的一种重要方式，对于高性能的 Web Server 可以从以下几个指标入手： 最大并发连接数 响应时间 吞吐量 吞吐量 = 最大并发连接数/响应时间，所以提高一个 Web Server 的性能就是不断提高这些指标，但是要提高这些指标有一个非常重要的前置条件，就是不能靠单纯提高硬件的性能来达到高性能的要求，而是在逼近硬件性能极限的条件下通过不断优化软件来实现高性能的 Web Server。 11.2 高性能的 web server 模型 从单进程模型到 IO 多路复用技术，每种模型都有其自身的特点： 单进程模型实现简单，适用场景非常有限 多进程模型有一定的并发能力，实现简单，但是数据共享很麻烦 多线程模型有一定的并发能力，数据共享比较简单，但是会引入\b锁机 制，实现稍微复杂， IO 多路复用技术可以有效解决 IO 等待的问题，但是要利用多核优势还需要多进程或多线程。 所以现实中 web server 的实现都是这几种模型的综合应用，组合起来一般包括： 单线程 IO 多路复用技术 + 单进程(单进程模型) 单线程 IO 多路复用技术 + 多线程(单进程多线程) 多线程/多进程 IO 多路复用技术 + 多进程/多线程(多进程或多进程多线程模型) 多线程/多进程 + 异步 IO 不论是采用哪种模型详细处理流程大致如下： 不同模型的差异在于 accept 操作、business process、IO 操作是单个线程完成还是多个线程或多个进程完成。 单线程 IO 多路复用技术 + 单进程(单进程模型) 这种模型中，accept 操作、业务逻辑处理、IO 处理都是在一个进程中单线程下处理完成，典型如 tornado、redis。 单线程 IO 多路复用技术 + 多线程(单进程多线程) 这种模型中，accept 操作是单独的线程，IO以及业务逻辑处理按照不同场景需求分别交给单独的线程或同一个线程来处理，典型如 gunicorn 支持单进程多线程。 多线程/多进程 IO 多路复用技术 + 多进程/多线程 这种模型中，accept 操作是多个的线程或进程实现，IO以及业务逻辑处理也是由多个线程完成或进程完成，典型如 nginx 多进程，每个进程都有自己的 accept 多线程/多进程 + 异步 IO linux 中真正的异步 IO 实现用的还比较少，在此我们不做深入介绍，感兴趣的读者可以看看 Windows 平台的异步 IO 实现 https://docs.microsoft.com/en-us/windows/desktop/fileio/i-o-completion-ports 11.3 高性能 web server 的实现原则 虽然现在开源实现中各种 web server 细节差异非常大，但总体来说所有的 web server 在实现高性能要求时都遵守类似的原则： 尽量避免数据拷贝(Zero Copy) 尽量避免上下文切换(Context Switchs） 尽量减少内存分配次数(Memory Allocation) 尽量减少锁的使用(Lock) Zero Copy 没有数据拷贝的 web server 几乎是不可能实现的，代码复杂度高，而且很难经得起真实场景的考验，即使应用程序能够做到不拷贝数据，但是内核也需要从网络中拷贝数据。既然理想的状态难以达到，我们可以折中实现：尽量避免大对象的拷贝。 Context Switchs 在 linux 系统中，如果 web server 进程过多或线程过多都会造成系统在处理请求时需要频繁进行上下文切换，因而一个高性能 web server 的开启的进程数或线程数需要严格控制，一般不大于 CPU 的核心数。 Memory Allocation 大量而且频繁的内存分配是高性能的一个致命杀手，为了尽量避免在 web server 大量请求处理过程中内存的频繁分配，可以考虑使用： 预分配 对象的复用比如使用池化技术 避免使用锁 Lock 在多线程或多进程的 web server 中锁由于潜在的多线程竞争，锁的使用不可避免，因而我们只能尽量去控制在不需要锁的地方尽量不使用，在需要使用锁的地方控制好锁的粒度。 11.4 高性能 web server 之 Nginx Nginx 是什么以及 nginx 能够做什么不是本文讨论的重点，本文的重点是 nginx 使用的 web server 模型，即本系列的主题。 11.4.1 Master-worker 模型 Nginx 采用的是 master-worker 模型： master 进程是在启动 nginx 时创建的第一个进程，一般以超级权限建立，master 主要负责： 读取和校验配置 创建、绑定和关闭 socket 创建、终止、维护 worker 进程 无中断情况下重新更新配置 重新开启日志文件 等等 worker 进程主要负责： connection 的 accept 和 process 反向代理 请求过滤 nginx 支持的常规功能等等 也就是说 master 进程在启动时会创建和绑定到指定的 socket，然后 master 进程根据配置(默认选择各个平台最高效的网络 IO 模型，linux 是 epoll) fork 出指定数量的 worker 进程，worker 进程负责通过 master 进程绑定的 socket 进行 accept 和 process 来自 client 的 connection，各个 worker 进程间通过共享内存的方式进行通信，虽然各个 worker 都可以进行 connection 的 accept 但是 nginx 通过文件锁以及各个 worker 进程当前处理 connection 的数量进行评估仅仅选择一个 worker 进程进程新 connection 的 accept。 Nginx 的 worker 进程是默认单线程的，也就是 worker 进程是单进程模型+IO 多路复用技术，各个 worker 进程又组成了 多进程模型+IO 多路复用技术，不过任意时刻，只有一个 worker 进程能够 accept 新的 connection。 Nginx 的这种 master-worker 的模型能够充分发挥现代 CPU 多核的能力，下图来自网络的 nginx 架构图： 11.4.2 Nginx 的高性能 Nginx 在 web server 领域的地位是久经考验的，这得益于它的使用简单、性能强劲，nginx 为什么达到如此高的性能标准，结合本系列介绍的各个主题我们一起来分析一下。 C 语言 Nginx 是 C 语言编写的，由于 C 语言是最接近系统的底层的高级语言，C 语言的性能几乎是所有高级语言中最好的。 单进程单线程模型 不同于传统的多进程以及多线程 web server，nginx 中一个 connection 从 accept 到 process 都是在一个进程的一个线程中完成，完全不需要进行进程或线程的上下文切换，也不需要锁的开销，这就最大程度上保证了单个进程几乎能够跑满单核 CPU（可以在 nginx 配置中指定 worker 进程绑定到具体的 CPU），最大程度上减少对 CPU 时钟的浪费。 各个平台的高效事件模型 Nginx 会选择各个平台最高效的网络模型来实现对网络事件的充分利用，比如 linux 下默认是 epoll，macOS 的 kqueue。 持续的优化 Nginx 从发布第一版到现在已经过去 10 多年了，在社区以及 nginx 官方的不断努力之下，nginx 持续不断地改进，包括内存分配、锁的利用、尽量少的系统调用等等技术的优化，使得 nginx 越来越高效，特性越来越丰富。 11.5 高性能 Web Server 之 Tornado Tornado 是一个 Python web framework，同时也是一个 networking library，tornado 在 Python 2 上借力于各个平台的 IO 多路复用技术（epoll on linux，kqueue on BSD and Mac OS X ）实现了一个基于事件的 IOLoop ，在 Python 3 上 tornado 的 IOLoop 封装的是 Python 3 的 asyncio 。 11.5.1 单线程的 IOLoop Tornado 的 IOLoop 默认情况下是基于单线程的 ，虽然 tornado 支持在多线程中使用 IOLoop ，但是最终的 IO 处理最终还是要在 IOLoop 所在的线程中执行。 由于 torando 是单线程的，一个显著的问题就是如果一个请求中有阻塞调用，会导致整个 tornado 进程发生阻塞，从而让其他请求变慢甚至不可响应，因此 tornado 推荐在生产环境中启用多个 tornado 进程，每个进程的中的阻塞调用使用多线程或消息队列等异步调用模型。 11.5.2 Tornado 的高性能 借助于非阻塞的 network IO ，tornado 能够轻松应对大规模的并发连接，因而 tornado 的非常适合在 Long Polling 和 WebSockets 等其他需要长连接的应用程序中使用。 为什么 tornado 能够轻松扩展的数万的连接请求，结合本系列的主题我们来一起分析一下。 基于事件循环的 IOLoop 同 nginx 一样，tornado 利用了各个平台的高效的提供的高效的网络 IO 多路复用机制，使得在处理网络 IO 事件非常高效。 单进程单线程模型 单线程模型能够有效减少由于上下文切换导致的开销，能够充分利用单核 CPU，而且单线程模型使得编写应用程序无需担心共享资源的从而导致的锁开销，使得资源利用率更高。 Python 的协程支持 Tornado 在 Python 2 中实现了协程，在 Python 3 中使用原生的协程，借助协程在 tornado 中可以轻松编写基于多个网络 IO 的并发程序，使得应用程序在等待网络 IO 同时能够处理更多其他的应用程序逻辑，包括处理更多的连接请求。 异步 tornado 在处理网络 IO 的时候尽量使用异步执行，比如 tornado 提供的异步 httpClient 在处理 http 请求时可以结合 tornado 提供的携程轻松完成异步 http 请求，可以同时并发执行多个网络 IO 。 © 三月沙 all right reserved，powered by GitbookUpdated at 2019-05-23 11:02:59 "}}